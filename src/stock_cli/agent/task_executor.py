"""ä»»åŠ¡æ‰§è¡Œå™¨"""

import asyncio
import time
from typing import Optional, Callable, Dict, Any, List, Awaitable

from ..core.types import Task
from ..agent.kernel import AgentKernel
from ..core.app_state import app_state
from ..agent.runtime import current_model
from rich import print as rich_print
from rich.console import Console

console = Console()


class TaskExecutor:
    """ä»»åŠ¡æ‰§è¡Œå™¨ï¼Œè´Ÿè´£ä»»åŠ¡çš„åˆ›å»ºå’Œæ‰§è¡Œ"""
    
    def __init__(self, kernel: AgentKernel):
        self.kernel = kernel

    async def run_agent_with_interrupt(
        self,
        question: str,
        *,
        stream: bool = True,
        capture_steps: bool = False,
        minimal: bool = False,
        enable_interrupt: bool = False,
        use_persistent_context: bool = False,
    ) -> dict:
        """è¿è¡Œå•ä¸ª Agent ä»»åŠ¡å¹¶è¿”å›ç»“æœ/æ¨ç†æ‘˜è¦ï¼Œæ”¯æŒè¿è¡Œæ—¶ä¸­æ–­ã€‚"""
        # é‡ç½®ä¸­æ–­æ ‡å¿—
        app_state.reset_interrupt()

        start_t = time.time()
        progress_lines: List[str] = []

        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
        async def on_progress(chunk: str):
            # æ£€æŸ¥æ˜¯å¦æ”¶åˆ°ä¸­æ–­è¯·æ±‚
            if app_state.interrupt_requested:
                raise asyncio.CancelledError("ç”¨æˆ·ä¸­æ–­")

            if chunk.startswith("[Stream]"):
                text = chunk.replace("[Stream]", "")
                # ä½¿ç”¨printç›´æ¥è¾“å‡ºï¼Œé¿å…Richçš„æ½œåœ¨æˆªæ–­é—®é¢˜
                print(text, end="", flush=True)
            elif not minimal and chunk.startswith("[StreamThinking]"):
                text = chunk.replace("[StreamThinking]", "")
                console.print(f"[dim]{text}[/dim]", end="", highlight=False)
            elif not minimal and chunk.startswith("[StreamAction]"):
                text = chunk.replace("[StreamAction]", "")
                console.print(f"[dim]{text}[/dim]", end="")
            elif not minimal and chunk.startswith("[ThinkingHeader]"):
                console.print("\n[dim]ğŸ’­ thinking: [/dim]", end="")
            elif not minimal and chunk.startswith("[ActionHeader]"):
                console.print("\n[dim]âš¡ action: [/dim]", end="")
            elif not minimal and chunk.startswith("[FinalAnswerHeader]"):
                # æ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆæ ‡é¢˜å’Œä¸Šæ–¹æ¨ªçº¿
                title = "âœ… æœ€ç»ˆç­”æ¡ˆ"
                console.print(f"\n[bold green]{title}[/bold green]")
                console.print("â”€" * 50)
            elif not minimal and chunk.startswith("[StreamFinalAnswer]"):
                text = chunk.replace("[StreamFinalAnswer]", "")
                # æœ€ç»ˆç­”æ¡ˆä½¿ç”¨æ­£å¸¸é¢œè‰²æ˜¾ç¤ºï¼Œä¸ç”¨dim
                print(text, end="", flush=True)
            elif not minimal and chunk.startswith("[FinalAnswerEnd]"):
                # æœ€ç»ˆç­”æ¡ˆç»“æŸï¼Œæ˜¾ç¤ºä¸‹æ–¹æ¨ªçº¿
                console.print(f"\n{'â”€' * 50}")
            elif not minimal and chunk.startswith("[StreamThinking]"):
                text = chunk.replace("[StreamThinking]", "")
                console.print(f"[dim]{text}[/dim]", end="")
            elif not minimal and chunk.startswith("[Thinking]"):
                console.print(
                    f"\n[dim]ğŸ’­ thinking: {chunk.replace('[Thinking]', '').strip()}[/dim]"
                )
            elif not minimal and chunk.startswith("[Action]"):
                console.print(
                    f"\n[dim]âš¡ action: {chunk.replace('[Action]', '').strip()}[/dim]"
                )
            # è¿‡æ»¤æ‰åŸå§‹çš„ReActå…³é”®è¯
            elif not minimal and chunk.strip() in ["Action", "Thought", "Final Answer"]:
                pass  # å¿½ç•¥è¿™äº›åŸå§‹å…³é”®è¯
            if capture_steps and (
                chunk.startswith("[Thinking]") or chunk.startswith("[Action]")
            ):
                progress_lines.append(chunk)

        # æ ¹æ®æ¨¡å¼å†³å®šä½¿ç”¨ä»€ä¹ˆä¸Šä¸‹æ–‡
        if use_persistent_context:
            # chat æ¨¡å¼ï¼šä½¿ç”¨æŒä¹…ä¸Šä¸‹æ–‡
            if "conversation_history" not in app_state.persistent_context:
                app_state.persistent_context["conversation_history"] = []

            # å°†å½“å‰é—®é¢˜æ·»åŠ åˆ°å¯¹è¯å†å²
            app_state.persistent_context["conversation_history"].append(
                {"role": "user", "content": question}
            )

            task = Task(description=question, context=app_state.persistent_context)
        else:
            # ask æ¨¡å¼ï¼šä½¿ç”¨ç©ºä¸Šä¸‹æ–‡
            task = Task(description=question, context={})

        # åˆ›å»º agent æ‰§è¡Œä»»åŠ¡
        app_state.current_task = asyncio.create_task(
            self.kernel.execute_task(task, progress_cb=on_progress, stream=stream)
        )

        try:
            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            answer = await app_state.current_task

            # å¦‚æœä½¿ç”¨æŒä¹…ä¸Šä¸‹æ–‡ï¼Œä¿å­˜AIçš„å›ç­”åˆ°å¯¹è¯å†å²
            if use_persistent_context and answer:
                app_state.persistent_context["conversation_history"].append(
                    {"role": "assistant", "content": answer}
                )

                # é™åˆ¶å¯¹è¯å†å²é•¿åº¦ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
                max_history_pairs = 8  # ä¿ç•™æœ€è¿‘8è½®å¯¹è¯ï¼ˆ16æ¡æ¶ˆæ¯ï¼‰
                if len(app_state.persistent_context["conversation_history"]) > max_history_pairs * 2:
                    # ä¿ç•™æœ€æ–°çš„å¯¹è¯ï¼Œåˆ é™¤æœ€æ—§çš„
                    app_state.persistent_context["conversation_history"] = app_state.persistent_context[
                        "conversation_history"
                    ][-(max_history_pairs * 2) :]

        except asyncio.CancelledError:
            if app_state.interrupt_requested:
                raise Exception("ä»»åŠ¡å·²è¢«ç”¨æˆ·ä¸­æ–­")
            else:
                raise Exception("ä»»åŠ¡å·²è¢«åœæ­¢")
        finally:
            app_state.current_task = None
            app_state.reset_interrupt()

        latency = round(time.time() - start_t, 3)
        token_usage = task.context.get("token_usage") or {}

        return {
            "answer": answer,
            "model": current_model() or "unknown",
            "latency": latency,
            "reasoning": [],  # ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¼å¼åŒ–æ¨ç†æ­¥éª¤
            "_raw_reasoning": progress_lines,
            "tokens": token_usage,
        }